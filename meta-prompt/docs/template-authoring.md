# Template Authoring Guide

Learn how to create high-quality prompt templates that integrate seamlessly with the meta-prompt optimization infrastructure.

---

## Table of Contents

1. [When to Create a New Template](#when-to-create-a-new-template)
2. [Template Anatomy](#template-anatomy)
3. [Step-by-Step Creation Process](#step-by-step-creation-process)
4. [Variable Naming Conventions](#variable-naming-conventions)
5. [Writing Effective Template Bodies](#writing-effective-template-bodies)
6. [Classification Keyword Selection](#classification-keyword-selection)
7. [Testing Your Template](#testing-your-template)
8. [Real Example Walkthrough](#real-example-walkthrough)
9. [Best Practices](#best-practices)
10. [Common Pitfalls](#common-pitfalls)

---

## When to Create a New Template

### Frequency Thresholds

Create a new template when:

1. **High Frequency:** You encounter the same pattern 15+ times
2. **Clear Pattern:** The task structure is consistent and repeatable
3. **Token Savings:** The pattern consumes >1000 tokens when generated by LLM
4. **Distinct Category:** It doesn't fit existing templates well (confidence <70%)

### Don't Create a Template If:

- The pattern appears <10 times
- Each instance is highly unique
- The pattern is too simple (<3 variables)
- It overlaps heavily with existing templates

### Current Template Capacity

- **Current:** 6 templates
- **Recommended maximum:** 15-20 templates before classification complexity increases
- **Sweet spot:** 8-12 templates for most projects

---

## Template Anatomy

Every template consists of two parts:

### 1. YAML Frontmatter (Metadata)

```yaml
---
template_name: your-template-name
category: broad-category
keywords: [keyword1, keyword2, keyword3, ...]
complexity: simple|intermediate|complex
variables: [VAR1, VAR2, VAR3, ...]
version: 1.0
description: One-line description of what this template does
variable_descriptions:  # Optional but recommended
  VAR1: "Explanation of what this variable represents"
  VAR2: "Explanation of what this variable represents"
---
```

### 2. Template Body (Prompt Content)

```markdown
Your prompt content goes here.

Use {$VARIABLE_NAME} placeholders for dynamic content.

<structured>
You can use XML tags for organization.
</structured>

Include clear instructions and examples.
```

---

## Step-by-Step Creation Process

### Step 1: Identify the Pattern

Collect 3-5 examples of the task you want to templatize:

**Example Pattern:** Sentiment Analysis
```
"Analyze the sentiment of this product review: [review text]"
"What's the sentiment of this tweet: [tweet]"
"Determine if this email is positive, negative, or neutral: [email]"
```

**Common structure:**
- Action: Analyze sentiment
- Input: Text to analyze
- Output: Positive/negative/neutral classification

### Step 2: Extract Variables

Identify what changes between instances:

- `TEXT_TO_ANALYZE`: The content being analyzed
- `OUTPUT_FORMAT`: (optional) How to present the result

### Step 3: Choose Template Name and Category

**Template name:** `sentiment-analysis` (lowercase, hyphenated)
**Category:** `analysis` (groups with similar templates)

### Step 4: Select Keywords

Keywords should match how users naturally describe the task:

**Strong indicators:** (75% confidence)
- "sentiment"
- "emotion"
- "feeling"

**Supporting keywords:** (8% each)
- "positive", "negative", "neutral"
- "analyze", "determine", "classify"
- "review", "tweet", "feedback", "email"

### Step 5: Create Template File

```bash
cd templates
cp custom.md sentiment-analysis.md
```

Edit the file:

```yaml
---
template_name: sentiment-analysis
category: analysis
keywords: [sentiment, emotion, feeling, positive, negative, neutral, analyze, tone, mood]
complexity: simple
variables: [TEXT_TO_ANALYZE]
version: 1.0
description: Analyze sentiment of text as positive, negative, or neutral
variable_descriptions:
  TEXT_TO_ANALYZE: "The text content to analyze (review, tweet, email, etc.)"
---

You will analyze the sentiment of the provided text.

<text>
{$TEXT_TO_ANALYZE}
</text>

<instructions>
1. Read the text carefully
2. Identify emotional tone and word choice
3. Classify as: POSITIVE, NEGATIVE, or NEUTRAL
4. Provide brief justification
</instructions>

<output_format>
**Sentiment:** [POSITIVE/NEGATIVE/NEUTRAL]
**Confidence:** [High/Medium/Low]
**Reasoning:** [Brief explanation]
</output_format>

<examples>
**Example 1:**
Text: "This product exceeded my expectations! Fast shipping too."
Sentiment: POSITIVE
Confidence: High
Reasoning: Enthusiastic language ("exceeded expectations"), exclamation point, additional positive note about shipping.

**Example 2:**
Text: "The item arrived. It works as described."
Sentiment: NEUTRAL
Confidence: High
Reasoning: Factual statements without emotional language. No positive or negative indicators.
</examples>

Begin your analysis immediately without preamble.
```

### Step 6: Update Classification Logic

Edit `agents/scripts/template-selector-handler.sh` to add classification keywords for your new template.

### Step 7: Validate

```bash
tests/validate-templates.sh sentiment-analysis
```

**Expected output:**
```
=== Template Validation ===
Validating: sentiment-analysis
  ✓ Has valid frontmatter
  ✓ Has required field: template_name
  ✓ Has required field: category
  ✓ Has required field: keywords
  ✓ Has required field: complexity
  ✓ Has required field: variables
  ✓ Has required field: version
  ✓ Has required field: description
  ✓ All declared variables used in template
  ✓ All used variables declared
  ✓ XML tags are balanced
  ✓ Template has content
PASSED: sentiment-analysis
```

### Step 8: Test Classification

Test that your template is being selected correctly:

```bash
/prompt --return-only "Analyze the sentiment of this product review"
```

### Step 9: Test Full Workflow

```bash
# Run complete test suite
tests/test-integration.sh

# Test actual usage
/prompt "Analyze the sentiment of this movie review: [review text]"
```

### Step 11: Update Documentation

1. Update `docs/architecture-overview.md` (template table)
2. Update `README.md` (template list)
3. Commit with descriptive message

---

## Variable Naming Conventions

### Naming Rules

1. **ALL_CAPS_SNAKE_CASE:** `USER_INPUT`, `DOCUMENT_TEXT`, `API_ENDPOINT`
2. **Descriptive:** `TEXT_TO_ANALYZE` not `TEXT`, `SOURCE_CODE` not `CODE`
3. **Consistent:** Use same name for same concept across templates
4. **No abbreviations:** `CLASSIFICATION_CRITERIA` not `CLASS_CRIT`

### Common Variable Names

| Variable | Use Case | Example |
|----------|----------|---------|
| `ITEM1`, `ITEM2` | Comparisons | Simple classification |
| `DOCUMENT`, `QUESTION` | Q&A | Document analysis |
| `TASK_REQUIREMENTS`, `TARGET_PATTERNS` | Code work | Refactoring |
| `TASK_DESCRIPTION`, `AVAILABLE_FUNCTIONS` | Tool use | Function calling |
| `ROLE_DESCRIPTION`, `CONTEXT`, `RULES` | Agents | Interactive dialogue |
| `TEXT_TO_ANALYZE`, `OUTPUT_FORMAT` | Analysis | Sentiment, summarization |

### Variable Descriptions

Always include `variable_descriptions` in frontmatter:

```yaml
variable_descriptions:
  SOURCE_CODE: "The code to be analyzed or refactored (function, class, or file)"
  MODIFICATION_GOAL: "What to change (e.g., 'improve performance', 'add error handling')"
```

This helps users understand what to provide for each variable.

### Variable Naming Patterns

To maintain consistency across templates, follow these naming patterns:

#### First Variable: The Primary Input

Choose a pattern that clearly indicates what type of input the template expects:

- **`SOURCE_*`** - For inputs that are the source material to work with:
  - `SOURCE_CODE` (code to refactor)
  - `SOURCE_DATA` (data to extract from)

- **`*_TO_*`** - For inputs describing transformation or analysis:
  - `CODE_TO_TEST` (code to generate tests for)
  - `TEXT_TO_ANALYZE` (text to analyze)

- **`PATHS`** - For file/directory references:
  - `PATHS` (files or directories to process, defaults to relevant files if not specified)

- **`INPUT_*`** - For generic or diverse inputs:
  - `DOCUMENT` (for document Q&A)
  - `CODE_OR_CONTENT` (for flexible documentation generator)

#### Second Variable: The Action/Configuration

Describes what to do or how to do it:

- **`*_FRAMEWORK`** - For tool/framework selection:
  - `TEST_FRAMEWORK` (Jest, pytest, etc.)

- **`*_FOCUS`** - For narrowing scope:
  - `REVIEW_FOCUS` (security, performance, etc.)
  - `TEST_SCOPE` (unit, integration, etc.)

- **`*_TYPE`** - For categorization:
  - `DOC_TYPE` (API docs, README, etc.)

- **`*_CRITERIA`** - For evaluation standards:
  - `CLASSIFICATION_CRITERIA` (how to classify items)

#### Third Variable: The Output/Context

Specifies output format or additional context:

- **`OUTPUT_*`** - For output specification:
  - `OUTPUT_FORMAT` (JSON, CSV, markdown, etc.)

- **`*_CONVENTIONS`** or **`CONTEXT`** - For additional guidance:
  - `LANGUAGE_CONVENTIONS` (coding standards to apply)
  - `AUDIENCE` (who the output is for)

#### Pattern Examples from Existing Templates

| Template | Var 1 (Input) | Var 2 (Action) | Var 3 (Output/Context) | Additional |
|----------|---------------|----------------|------------------------|------------|
| test-generation | `CODE_TO_TEST` | `TEST_FRAMEWORK` | `TEST_SCOPE` | - |
| code-review | `PATHS` | `REVIEW_FOCUS` | `LANGUAGE_CONVENTIONS` | - |
| documentation-generator | `CODE_OR_CONTENT` | `DOC_TYPE` | `AUDIENCE` | - |
| data-extraction | `SOURCE_DATA` | `EXTRACTION_TARGETS` | `OUTPUT_FORMAT` | - |
| simple-classification | `ITEM1` | `ITEM2` | `CLASSIFICATION_CRITERIA` | - |
| document-qa | `DOCUMENT` | `QUESTION` | - | - |

#### Guidelines for New Templates

1. **Be consistent within template categories:**
   - All "analysis" templates should use similar patterns
   - All "generation" templates should use similar patterns

2. **First variable should clearly indicate input type:**
   - Use `*_TO_*` when the action is clear from context
   - Use `SOURCE_*` when emphasizing the source material
   - Use specific names (`DOCUMENT`, `CODE`) when type is unambiguous

3. **Avoid mixing patterns unnecessarily:**
   - If most templates use `*_TO_*`, prefer that pattern
   - If you deviate, document why in the template description

4. **Keep variable counts consistent:**
   - Simple templates: 2-3 variables
   - Intermediate templates: 3-4 variables
   - Complex templates: 3-5 variables

---

## Writing Effective Template Bodies

### Structure Your Template

```markdown
[1. Context setting - what role is the LLM playing?]
You are a [role] helping with [task].

[2. Input presentation - XML tags recommended]
<input_name>
{$VARIABLE}
</input_name>

[3. Clear instructions - numbered or bulleted]
**Instructions:**
1. First step
2. Second step
3. Third step

[4. Output format - show exact format expected]
**Format:**
<output>
[Expected structure]
</output>

[5. Examples - concrete demonstrations]
**Example:**
Input: [example input]
Output: [example output]

[6. Edge case handling - what to do if...]
If [condition], then [action].

[7. Final directive - how to begin]
Begin your response immediately without preamble.
```

### Best Practices for Template Body

**DO:**
- ✓ Use XML tags for structured input/output
- ✓ Provide concrete examples
- ✓ Be explicit about output format
- ✓ Include edge case handling
- ✓ Use clear, imperative language
- ✓ Number steps for complex processes
- ✓ Include TodoWrite step for complex templates (7+ steps or multi-file changes)
- ✓ Specify when to update TodoWrite progress (mark completed immediately after finishing each task)

**DON'T:**
- ✗ Use vague instructions ("try to", "maybe")
- ✗ Assume the LLM knows context
- ✗ Mix multiple unrelated tasks
- ✗ Over-explain (keep it concise)
- ✗ Use placeholder variables in examples
- ✗ Forget TodoWrite for complex, multi-step templates

### Complexity Levels

**Simple** (≤3 steps):
```markdown
1. Read input
2. Perform action
3. Output result
```

**Intermediate** (4-6 steps):
```markdown
1. Read input
2. Extract information
3. Analyze against criteria
4. Format output
5. Cite sources
```

**Complex** (7+ steps):
```markdown
1. Use TodoWrite to plan the work (search, identify files, implementation steps, testing)
2. Search for target patterns
3. Read files
4. Make modifications
5. Test changes
6. Update TodoWrite as you progress (mark completed immediately after finishing each task)
7. Mark all todos as completed when done
```

**Important:** Complex templates should ALWAYS include TodoWrite instructions as Step 1. This ensures sub-agents:
- Break down work into trackable tasks
- Maintain progress visibility
- Complete tasks systematically
- Verify all work is finished

---

## Classification Keyword Selection

### Keyword Strategy

**Goal:** Achieve 90%+ correct classifications with 70% confidence threshold.

### Types of Keywords

**1. Strong Indicators** (75% base confidence)
- Unique to this template
- Rare in other contexts
- Examples: "refactor" → code, "sentiment" → analysis, "tutor" → dialogue

**2. Supporting Keywords** (8% each)
- Common in this domain
- Add confidence when combined
- Examples: "code", "function", "file" for code templates

**3. Avoid**
- Generic words ("the", "a", "help")
- Ambiguous terms that appear in multiple templates
- Rare synonyms users won't use

### Keyword Selection Process

1. **Collect real examples:** 20+ task descriptions from actual use
2. **Extract common words:** What words appear in 80%+ of examples?
3. **Filter for specificity:** Remove words that appear in other templates
4. **Test with DEBUG:** Verify classification confidence
5. **Iterate:** Add/remove keywords based on real usage

### Testing Keywords

```bash
# Test multiple variations using /prompt --return-only
/prompt --return-only "refactor the code"
/prompt --return-only "modify the function"
/prompt --return-only "update the class"

# All should route to code-refactoring template
```

---

## Testing Your Template

### Validation Tests (Automated)

```bash
# Validates structure, variables, XML tags
tests/validate-templates.sh your-template-name
```

**What it checks:**
- ✓ YAML frontmatter present and valid
- ✓ All required fields exist
- ✓ Variables declared match variables used
- ✓ XML tags balanced
- ✓ Template has non-empty content

### Classification Tests

Test that tasks route correctly using the `/prompt` command:

```bash
/prompt --return-only "task description"
```

**Success criteria:**
- Your template is selected for matching tasks
- Tasks that don't match fall back to another template or custom

### Manual Testing

```bash
# Test full workflow
/prompt "actual task description"

# Verify:
# 1. Correct template selected
# 2. Variables properly substituted
# 3. Output makes sense
```

---

## Real Example Walkthrough

Let's create a "code-comparison" template from scratch.

### Step 1: Identify the Pattern

Users frequently ask:
- "What's the difference between these two code snippets?"
- "Compare implementation A vs implementation B"
- "Which approach is better: [code1] or [code2]?"

### Step 2: Extract Variables

- `CODE_SNIPPET_1`: First code sample
- `CODE_SNIPPET_2`: Second code sample
- `COMPARISON_CRITERIA`: What to compare (performance, readability, correctness)

### Step 3: Create Template

**File:** `templates/code-comparison.md`

```yaml
---
template_name: code-comparison
category: analysis
keywords: [difference, compare, versus, vs, better, worse, implementation, approach, alternative]
complexity: intermediate
variables: [CODE_SNIPPET_1, CODE_SNIPPET_2, COMPARISON_CRITERIA]
version: 1.0
description: Compare two code snippets across specified criteria
variable_descriptions:
  CODE_SNIPPET_1: "First code snippet to compare"
  CODE_SNIPPET_2: "Second code snippet to compare"
  COMPARISON_CRITERIA: "What to compare (e.g., performance, readability, maintainability)"
---

You are a code review expert comparing two implementations.

<code_1>
{$CODE_SNIPPET_1}
</code_1>

<code_2>
{$CODE_SNIPPET_2}
</code_2>

<criteria>
{$COMPARISON_CRITERIA}
</criteria>

**Analysis Process:**
1. Understand what each code snippet does
2. Evaluate both against the comparison criteria
3. Identify strengths and weaknesses of each
4. Provide a recommendation if applicable

**Output Format:**
### Code Snippet 1
- Strengths: [list]
- Weaknesses: [list]
- Rating: [1-5 on criteria]

### Code Snippet 2
- Strengths: [list]
- Weaknesses: [list]
- Rating: [1-5 on criteria]

### Recommendation
[Which is better for the given criteria, or when to use each]

**Important:**
- Be objective and specific
- Cite actual code when explaining
- Consider edge cases and trade-offs

Begin your comparison immediately.
```

### Step 4: Update Classification

Edit `agents/scripts/template-selector-handler.sh` to add keywords for your new template.

Add strong and supporting keywords:
- Strong keywords provide 75% base confidence
- Supporting keywords add 8% each

### Step 5: Validate

```bash
tests/validate-templates.sh code-comparison
# Should pass all checks
```

### Step 6: Test

```bash
/prompt --return-only "Compare these two implementations for readability"
```

### Step 7: Document

Update:
- `README.md` - Add to template list
- `docs/architecture-overview.md` - Add to template table
- Git commit with clear message

---

## Best Practices

### Template Design

1. **Single Responsibility:** Each template should do one thing well
2. **Clear Examples:** Include 2-3 concrete examples in template body
3. **Explicit Output Format:** Show exactly what output should look like
4. **Edge Case Handling:** Address "what if" scenarios
5. **Progressive Disclosure:** Simple instructions first, details later

### Keyword Selection

1. **User Language:** Use words users naturally say, not technical jargon
2. **Balanced Coverage:** 3-5 strong indicators + 8-12 supporting keywords
3. **Test Variations:** Try synonyms and phrasings
4. **Monitor False Positives:** If template triggers incorrectly, refine keywords

### Variables

1. **Minimal Set:** Fewest variables possible (3-5 ideal)
2. **Optional Variables:** Use conditional text for optional elements
3. **Clear Names:** Self-documenting variable names
4. **Descriptions:** Always explain what each variable represents

### Maintenance

1. **Version Bumps:** Increment version when changing variables or structure
2. **Changelog:** Document changes in git commit messages
3. **Backwards Compatibility:** Don't break existing usage if possible
4. **Quarterly Review:** Revisit template effectiveness every 3 months

---

## Common Pitfalls

### Pitfall 1: Too Many Variables

**Problem:** Template with 8+ variables becomes hard to use
**Solution:** Combine related variables or split into multiple templates

### Pitfall 2: Overlapping Keywords

**Problem:** Multiple templates match the same task
**Solution:** Use more specific strong indicators, refine supporting keywords

### Pitfall 3: Too Generic

**Problem:** Template tries to cover too many use cases
**Solution:** Split into specialized templates (better UX)

### Pitfall 4: Too Specific

**Problem:** Template only works for one exact scenario
**Solution:** Generalize slightly or keep as custom (no template needed)

### Pitfall 5: Poor Examples

**Problem:** Examples in template are abstract or confusing
**Solution:** Use concrete, realistic examples from actual use cases

### Pitfall 6: Inconsistent Format

**Problem:** Template doesn't follow established patterns
**Solution:** Copy structure from existing templates, maintain consistency

### Pitfall 7: Missing Validation

**Problem:** Template deployed without testing
**Solution:** Always run validation + integration tests before committing

---

## Resources

### Template Examples
- Simple: `templates/simple-classification.md`
- Intermediate: `templates/document-qa.md`
- Complex: `templates/code-refactoring.md`

### Scripts
- Validation: `tests/validate-templates.sh`
- Classification: `agents/scripts/template-selector-handler.sh`
- Processing: `agents/scripts/prompt-optimizer-handler.sh`
- Testing: `tests/test-integration.sh`

### Documentation
- [Architecture Overview](architecture-overview.md) - Template system design
- [Design Decisions](design-decisions.md) - Why templates work this way
- [Infrastructure Guide](infrastructure.md) - Operational details

---

## Getting Help

**Questions? Issues?**
1. Review existing templates for inspiration
2. Use `/prompt --return-only` to test classification behavior
3. Run validation early and often
4. See [CONTRIBUTING.md](CONTRIBUTING.md) for support channels

Happy template authoring!
